{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "585de33a",
   "metadata": {},
   "source": [
    "### Convert text to vectors using Bag-of-Words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceb2bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c7d2586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fourscore and seven years ago our fathers brought forth, on this continent, a new nation, conceived \n",
      "in liberty, and dedicated to the proposition that all men are created equal. Now we are engaged in a great \n",
      "civil war, testing whether that nation, or any nation so conceived, and so dedicated, can long endure. We are \n",
      "met on a great battle-field of that war. We have come to dedicate a portion of that field, as a final \n",
      "resting-place for those who here gave their lives, that that nation might live. It is altogether fitting and \n",
      "proper that we should do this. But, in a larger sense, we cannot dedicate, we cannot consecrate—we cannot \n",
      "hallow—this ground. The brave men, living and dead, who struggled here, have consecrated it far above our poor \n",
      "power to add or detract. The world will little note, nor long remember what we say here, but it can never \n",
      "forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which \n",
      "they who fought here have thus far so nobly advanced. It is rather for us to be here dedicated to the great \n",
      "task remaining before us—that from these honored dead we take increased devotion to that cause for which they \n",
      "here gave the last full measure of devotion—that we here highly resolve that these dead shall not have died in \n",
      "vain—that this nation, under God, shall have a new birth of freedom, and that government of the people, by the \n",
      "people, for the people, shall not perish from the earth.\n"
     ]
    }
   ],
   "source": [
    "## Create a corpus\n",
    "speech = '''Fourscore and seven years ago our fathers brought forth, on this continent, a new nation, conceived \n",
    "in liberty, and dedicated to the proposition that all men are created equal. Now we are engaged in a great \n",
    "civil war, testing whether that nation, or any nation so conceived, and so dedicated, can long endure. We are \n",
    "met on a great battle-field of that war. We have come to dedicate a portion of that field, as a final \n",
    "resting-place for those who here gave their lives, that that nation might live. It is altogether fitting and \n",
    "proper that we should do this. But, in a larger sense, we cannot dedicate, we cannot consecrate—we cannot \n",
    "hallow—this ground. The brave men, living and dead, who struggled here, have consecrated it far above our poor \n",
    "power to add or detract. The world will little note, nor long remember what we say here, but it can never \n",
    "forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which \n",
    "they who fought here have thus far so nobly advanced. It is rather for us to be here dedicated to the great \n",
    "task remaining before us—that from these honored dead we take increased devotion to that cause for which they \n",
    "here gave the last full measure of devotion—that we here highly resolve that these dead shall not have died in \n",
    "vain—that this nation, under God, shall have a new birth of freedom, and that government of the people, by the \n",
    "people, for the people, shall not perish from the earth.'''\n",
    "\n",
    "print(speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d832b9",
   "metadata": {},
   "source": [
    "#### Cleaning the Text\n",
    "\n",
    "- Removing stopwords\n",
    "- Removing punctuations\n",
    "- Stemming and Lemmatization\n",
    "- Lower the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0a25a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6670052",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36fc6f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fourscore and seven years ago our fathers brought forth, on this continent, a new nation, conceived \\nin liberty, and dedicated to the proposition that all men are created equal.',\n",
       " 'Now we are engaged in a great \\ncivil war, testing whether that nation, or any nation so conceived, and so dedicated, can long endure.',\n",
       " 'We are \\nmet on a great battle-field of that war.',\n",
       " 'We have come to dedicate a portion of that field, as a final \\nresting-place for those who here gave their lives, that that nation might live.',\n",
       " 'It is altogether fitting and \\nproper that we should do this.',\n",
       " 'But, in a larger sense, we cannot dedicate, we cannot consecrate—we cannot \\nhallow—this ground.',\n",
       " 'The brave men, living and dead, who struggled here, have consecrated it far above our poor \\npower to add or detract.',\n",
       " 'The world will little note, nor long remember what we say here, but it can never \\nforget what they did here.',\n",
       " 'It is for us the living, rather, to be dedicated here to the unfinished work which \\nthey who fought here have thus far so nobly advanced.',\n",
       " 'It is rather for us to be here dedicated to the great \\ntask remaining before us—that from these honored dead we take increased devotion to that cause for which they \\nhere gave the last full measure of devotion—that we here highly resolve that these dead shall not have died in \\nvain—that this nation, under God, shall have a new birth of freedom, and that government of the people, by the \\npeople, for the people, shall not perish from the earth.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(speech)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a11e8f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fourscor seven year ago father brought forth contin new nation conceiv liberti dedic proposit men creat equal',\n",
       " 'engag great civil war test whether nation nation conceiv dedic long endur',\n",
       " 'met great battl field war',\n",
       " 'come dedic portion field final rest place gave live nation might live',\n",
       " 'altogeth fit proper',\n",
       " 'larger sens cannot dedic cannot consecr cannot hallow ground',\n",
       " 'brave men live dead struggl consecr far poor power add detract',\n",
       " 'world littl note long rememb say never forget',\n",
       " 'us live rather dedic unfinish work fought thu far nobli advanc',\n",
       " 'rather us dedic great task remain us honor dead take increas devot caus gave last full measur devot highli resolv dead shall die vain nation god shall new birth freedom govern peopl peopl peopl shall perish earth']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### PERFROM STEMMING\n",
    "stem_corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    only_alphas = re.sub('[^a-zA-Z]', ' ', sentences[i])     ### Keep only Alphabets\n",
    "    lower_words = only_alphas.lower()     ### Convert the remaining text to the same lower form\n",
    "    split_words = lower_words.split()     ### Split the lower words to form a list\n",
    "    stem_words = [stemmer.stem(word) for word in split_words if word not in set(stopwords.words('english'))]\n",
    "    final_text = ' '.join(stem_words)\n",
    "    stem_corpus.append(final_text)\n",
    "    \n",
    "stem_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9db7e1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fourscore seven year ago father brought forth continent new nation conceived liberty dedicated proposition men created equal',\n",
       " 'engaged great civil war testing whether nation nation conceived dedicated long endure',\n",
       " 'met great battle field war',\n",
       " 'come dedicate portion field final resting place gave life nation might live',\n",
       " 'altogether fitting proper',\n",
       " 'larger sense cannot dedicate cannot consecrate cannot hallow ground',\n",
       " 'brave men living dead struggled consecrated far poor power add detract',\n",
       " 'world little note long remember say never forget',\n",
       " 'u living rather dedicated unfinished work fought thus far nobly advanced',\n",
       " 'rather u dedicated great task remaining u honored dead take increased devotion cause gave last full measure devotion highly resolve dead shall died vain nation god shall new birth freedom government people people people shall perish earth']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### PERFROM LEMMATIZATION\n",
    "lemmat_corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    only_alphas = re.sub('[^a-zA-Z]', ' ', sentences[i])     ### Keep only Alphabets\n",
    "    lower_words = only_alphas.lower()     ### Convert the remaining text to the same lower form\n",
    "    split_words = lower_words.split()     ### Split the lower words to form a list\n",
    "    stem_words = [lemmatizer.lemmatize(word) for word in split_words if word not in set(stopwords.words('english'))]\n",
    "    final_text = ' '.join(stem_words)\n",
    "    lemmat_corpus.append(final_text)\n",
    "    \n",
    "lemmat_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659b7498",
   "metadata": {},
   "source": [
    "##### Here we can see that when we perform lemmatization we gert a proper meaning to the sentences. Thus we will go with lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b68cf13",
   "metadata": {},
   "source": [
    "#### Creating the BOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45549c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer    ### This library will help in obtaining the histogram and ordering them and fonally give us our matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51a32382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 2,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 3, 1,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 3, 0, 1, 1, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(lemmat_corpus).toarray()     ### Here we will get the sparse matrix \n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "514d88af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 93)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79e464e",
   "metadata": {},
   "source": [
    "> Here we have **10 sentences** and **93 represents the vectors** in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf312693",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
